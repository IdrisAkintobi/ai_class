{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSr6zpP3Vk5a"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/IdrisAkintobi/ai_class/blob/main/gen_ai/RAG/RAG_with_faiss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W859FP6Tb3-q"
   },
   "source": [
    "# Retrival Augmentated Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rd3MPZXv2j55"
   },
   "outputs": [],
   "source": [
    "# Login to huggingface\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a78e556"
   },
   "source": [
    "First, we define a list of facts about Nigeria that we will use as our knowledge base for the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fifty facts about Nigeria\n",
    "document_list = [\n",
    "    'Nigeria is located in West Africa, bordered by Benin, Chad, Cameroon, and Niger.',\n",
    "    'It is the most populous country in Africa, with over 220 million people (as of 2024).',\n",
    "    'Nigeria is the 7th most populous country in the world.',\n",
    "    'The president of Nigeria is Idris Akintobi Adejare. Sworn into office on 17th of April 2022',\n",
    "    'The country has over 250 ethnic groups, making it one of the most culturally diverse nations on Earth.',\n",
    "    'The official capital is Abuja, which became the capital in 1991, replacing Lagos.',\n",
    "    'Nigeria’s terrain includes rainforests, savannas, mountains, and a long coastline along the Gulf of Guinea (853 km).',\n",
    "    'The Niger River and Benue River are the two largest rivers, converging in the center of the country.',\n",
    "    'Nigeria has over 1,000 species of butterflies and more than 1,000 bird species.',\n",
    "    'The highest point in Nigeria is Chappal Waddi (also called Gangirwal), standing at 2,419 meters (7,936 ft) in the Adamawa Mountains.',\n",
    "    'Nigeria has several national parks, including Yankari National Park and Cross River National Park.',\n",
    "    'Nigeria gained independence from British colonial rule on October 1, 1960.',\n",
    "    'It became a republic in 1963, with Nnamdi Azikiwe as its first president.',\n",
    "    'Nigeria experienced a brutal civil war (the Biafran War) from 1967 to 1970 after the southeastern region attempted to secede.',\n",
    "    'The war resulted in an estimated 1–3 million deaths, mostly from starvation.',\n",
    "    'Nigeria was under military rule for much of the 1970s–1990s, with brief democratic transitions.',\n",
    "    'The return to democracy in 1999 marked the beginning of Nigeria’s current Fourth Republic.',\n",
    "    'Nigeria has had 8 heads of state since independence, including both civilian and military leaders.',\n",
    "    'Olusegun Obasanjo is the only person to have served as both military head of state (1976–1979) and elected president (1999–2007).',\n",
    "    'Nigeria is a federal republic composed of 36 states and the Federal Capital Territory (Abuja).',\n",
    "    'The Nigerian Constitution is based on the British parliamentary system but incorporates elements of U.S. federalism.',\n",
    "    'Nigeria has the largest economy in Africa by nominal GDP (as of 2024).',\n",
    "    'It is the largest oil producer in Africa and a founding member of OPEC.',\n",
    "    'Oil accounts for about 90% of Nigeria’s export earnings and roughly 50% of government revenue.',\n",
    "    'Despite oil wealth, Nigeria struggles with widespread poverty and economic inequality.',\n",
    "    'Nigeria is a major global producer of cocoa, rubber, palm oil, and yams.',\n",
    "    'The country is home to Africa’s largest stock exchange: the Nigerian Stock Exchange (now NGX Group).',\n",
    "    'Nigeria has one of the fastest-growing tech ecosystems in Africa, nicknamed “Silicon Lagoon” in Lagos.',\n",
    "    'Remittances from the Nigerian diaspora are among the highest in Africa.',\n",
    "    'Nigeria has vast untapped mineral resources, including coal, limestone, tin, and iron ore.',\n",
    "    'The country is working to diversify its economy away from oil dependency through initiatives like the “Economic Recovery and Growth Plan.”',\n",
    "    'Nigeria is known as the “Giant of Africa” due to its large population, economy, and cultural influence.',\n",
    "    'English is the official language, used for government, education, and media, but over 500 indigenous languages are spoken.',\n",
    "    'The three largest ethnic groups are the Hausa-Fulani (North), Yoruba (Southwest), and Igbo (Southeast).',\n",
    "    'Nigeria produces the second-largest film industry in the world by volume — Nollywood — after India’s Bollywood.',\n",
    "    'Nollywood releases over 2,500 films annually, many produced on low budgets and distributed via DVDs and streaming platforms.',\n",
    "    'Nigerian music genres like Afrobeat (pioneered by Fela Kuti), Afrobeats, Highlife, and Fuji are globally popular.',\n",
    "    'Fela Kuti, the father of Afrobeat, was a political activist whose music criticized corruption and military rule.',\n",
    "    'Nigeria hosted the 2019 African Games and will host the 2027 Africa Cup of Nations.',\n",
    "    'The annual Eyo Festival in Lagos and the New Yam Festival among the Igbo are major cultural events.',\n",
    "    'Traditional Nigerian clothing includes the agbada (men’s flowing robe), iro and buba (women’s wrap and blouse), and gele (head tie).',\n",
    "    'Nigeria has over 150 universities, including the University of Ibadan (founded in 1948), the first university in Nigeria.',\n",
    "    'Nigeria has produced Nobel laureates — Wole Soyinka (Literature, 1986) — the first African to win the prize in Literature.',\n",
    "    'Nigeria has one of the highest rates of mobile phone usage in Africa, with over 200 million active lines.',\n",
    "    'Nigerian engineers designed the first indigenous satellite, NigComSat-1, launched in 2007.',\n",
    "    'Nigeria has made strides in renewable energy, particularly solar power, despite unreliable grid infrastructure.',\n",
    "    'The Nigerian Institute of Medical Research (NIMR) is a leading center for tropical disease research.',\n",
    "    'Nigeria has produced renowned scientists like Professor Adebayo Adesina (former President of the African Development Bank) and Dr. Ifeyinwa Osunkwo (global HIV researcher).',\n",
    "    'The country has one of the youngest populations in the world — over 60% of Nigerians are under age 25.',\n",
    "    'Nigeria leads Africa in digital innovation startups, with fintech companies like Flutterwave and Paystack gaining international recognition.',\n",
    "    'Nigerian athletes have won Olympic medals in track and field, boxing, and weightlifting — notably sprinter Blessing Okagbare and boxer David Izonritei.',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f86cfa74"
   },
   "source": [
    "We need to install the `faiss-cpu` library, which is a library for efficient similarity search and clustering of dense vectors. This will be used to create an index for our embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install faiss-cpu library\n",
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b37b7ba"
   },
   "source": [
    "Import the necessary libraries: `faiss` for vector indexing, `torch` for tensor operations (used by the transformer model), and `transformers` for loading the pre-trained tokenizer and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import libraries\n",
    "import faiss\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJch8Fm9TscR"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1c7a2f73"
   },
   "source": [
    "Now we need to tokenized and generate embeddings for each sentense in the document array. These embeddings will represent the semantic meaning of each sentense in a vector space, allowing us to find similar sentense based on their meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ee32eff"
   },
   "source": [
    "Initialize a tokenizer and a pre-trained model from the `sentence-transformers` library. The tokenizer will convert text into numerical tokens, and the model will be used to generate embeddings from these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "BF3R9RS1WovE"
   },
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and model for generating embeddings\n",
    "llm_checkpoint = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "doc_tokenizer = AutoTokenizer.from_pretrained(llm_checkpoint)\n",
    "doc_model = AutoModel.from_pretrained(llm_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c81e81db"
   },
   "source": [
    "Define a helper function `mean_pooling` to perform mean pooling on the model's output. This is a common technique to get a single vector representation for a sentence from the token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94986a59"
   },
   "outputs": [],
   "source": [
    "# Function to perform mean pooling\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    # First element of model_output contains all token embeddings (It can also be accessed with (.last_hidden_state))\n",
    "    token_embeddings = model_output[0]\n",
    "    mask = attention_mask.unsqueeze(-1).float()\n",
    "    return (token_embeddings * mask).sum(1) / mask.sum(1).clamp(min=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea402333"
   },
   "source": [
    "Define a function `generate_embeddings` that  tokenized text and, returns the embeddings generated by the model after applying mean pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfbe7f54"
   },
   "outputs": [],
   "source": [
    "# Function to generate embeddings from tokenized text\n",
    "def generate_embeddings(text: str):\n",
    "    tokenized_text = doc_tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = doc_model(**tokenized_text)\n",
    "    # Mean pooling to get a single vector representation for the entire sentence\n",
    "    embeddings = mean_pooling(embeddings, tokenized_text['attention_mask'])\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFk1Nkoqi9Tv"
   },
   "outputs": [],
   "source": [
    "document_embeddings = []\n",
    "\n",
    "for doc in document_list:\n",
    "    doc_embedding = generate_embeddings(doc)\n",
    "    document_embeddings.append(doc_embedding)\n",
    "\n",
    "document_embeddings = torch.stack(document_embeddings).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4gQ6nQjYjvMh",
    "outputId": "b6d57b2d-2a25-4be3-e3f7-d694bc40c85e"
   },
   "outputs": [],
   "source": [
    "# Print document embeddings array size and shape (batch_size, sequence_length, hidden_size)\n",
    "print(document_embeddings.size)\n",
    "print(document_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sibpfdf_l55h"
   },
   "source": [
    "# Build the retrieval system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qGJNoP-pl0Qq",
    "outputId": "565b0559-49ce-47db-b3aa-9521f0a8b950"
   },
   "outputs": [],
   "source": [
    "# Remove the extra dimension (size) before adding to the index\n",
    "document_embeddings = document_embeddings.squeeze()\n",
    "print(document_embeddings.shape)  # print the new shape\n",
    "\n",
    "index = faiss.IndexFlatL2(document_embeddings.shape[1])\n",
    "index.add(document_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MwOBE4Tn5Zv"
   },
   "source": [
    "Create function to retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTHeU9aymsGo"
   },
   "outputs": [],
   "source": [
    "distance_threshold = 18\n",
    "\n",
    "\n",
    "def retrieve(query, k=3):\n",
    "    query_embedding = generate_embeddings(query)\n",
    "    query_embedding = query_embedding.cpu().detach().numpy()\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    # Get the actual documents based on the indices\n",
    "    indecies, distances = indices[0], distances[0]\n",
    "    retrieved_docs = [\n",
    "        document_list[idx]\n",
    "        for idx, dist in zip(indecies, distances, strict=False)\n",
    "        if dist <= distance_threshold\n",
    "    ]\n",
    "    return '\\n'.join(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NzTAHOsvhpR"
   },
   "source": [
    "# Build the generation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SibtC3UUvo9a"
   },
   "outputs": [],
   "source": [
    "# Instantiate generator model\n",
    "model_name = 'google/gemma-3-270m'\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set pad_token to eos_token (required for Gemma)\n",
    "if gen_tokenizer.pad_token is None:\n",
    "    gen_tokenizer.pad_token = gen_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W5KjTE1wxPIu",
    "outputId": "59c2a88e-614c-4e88-99d6-0b934ec7ab69"
   },
   "outputs": [],
   "source": [
    "# Create the generator function\n",
    "def generate_text(prompt, max_length=100):\n",
    "    # Step 1: Retrieve relevant context\n",
    "    knowledge = retrieve(prompt)\n",
    "\n",
    "    # Check if there is relevant context\n",
    "    if not knowledge:\n",
    "        return 'No relevant context found.'\n",
    "\n",
    "    # Step 2: Format prompt clearly using another common RAG template\n",
    "    formatted_prompt = f\"\"\"Context: {knowledge}\n",
    "\n",
    "Question: {prompt}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "    # Step 3: Tokenize input\n",
    "    inputs = gen_tokenizer(\n",
    "        formatted_prompt,\n",
    "        return_tensors='pt',\n",
    "        padding=False,  # No padding needed for single inference\n",
    "        truncation=True,  # Always truncate if too long\n",
    "        max_length=512,  # Respect model's context window\n",
    "    )\n",
    "\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']  # Already computed correctly by tokenizer\n",
    "\n",
    "    # Step 4: Generate response\n",
    "    output = gen_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        repetition_penalty=1.1,\n",
    "        temperature=0.1,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=gen_tokenizer.pad_token_id,  # Use explicitly set pad_token\n",
    "        do_sample=True,\n",
    "        eos_token_id=gen_tokenizer.eos_token_id,  # Explicit stop condition\n",
    "    )\n",
    "\n",
    "    # Step 5: Decode and return clean output\n",
    "    generated_text = gen_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text.split('Answer: ')[-1]\n",
    "\n",
    "\n",
    "# The model used will not generate a good result. This is only for testing\n",
    "query = 'Who is the president of Nigeria?'\n",
    "print(generate_text(query, 120))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "ai-class (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
